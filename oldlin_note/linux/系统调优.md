### 1、有关使用 CPU 资源的调优

#### 1.调整 nice 值改变进程优先级

在 LINUX 系统中，Nice 值的范围从-20 到+19（不同系统的值范围是不一样的）， 正值表示低优先级，负值表示高优先级，值为零则表示不会调整该进程的优先级。具有最高优先级的程序，其nice 值最低，所以在LINUX 系统中，值-20 使得一项任务变得非常重要；与之相反，如果任务的 nice 为+19，则表示它是一个高尚的、无私的任务，允许所有其他任务比自己享有宝贵的 CPU 时间的更大使用份额，这也就是nice 的名称的来意。

#### 2.调整平均负载

##### 2.1.了解平均负载

平均负载是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数， 也就是**平均活跃进程数**，它和 CPU 使用率并没有直接关系。这里我先解释下，**可运行状态**和**不可中断状态**。

所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。

不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。

比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题。

所以，<font color='red'>不可中断状态实际上是系统对进程和硬件设备的一种保护机制。</font>

**平均负载为多少时合理，取决于首先你要知道系统有几个 CPU**

##### 2.2.实验准备

stress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。

注：用到epel源，才能安装stress命令

[root@master yum.repos.d]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

[root@master yum.repos.d]# yum install -y stress



 sysstat工具 包含了常用的 Linux 性能工具，用来监控和分析系统的性能。

我们的案例会用到这个包的两个命令 mpstat 和 pidstat。

mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标。

pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。

##### 2.3.场景一：cpu密集型进程

首先，我们在第一个终端运行 stress 命令，模拟一个 CPU 使用率 100% 的场景：

[root@master ~]# stress --cpu 1 --timeout 600

接着，在第二个终端运行 uptime 查看平均负载的变化情况：

 -d 参数表示高亮显示变化的区域

 watch -d uptime

 ...,	load average: 1.00, 0.75, 0.39

最后，在第三个终端运行 mpstat 查看 CPU 使用率的变化情况：

 -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据

```
[root@master ~]# mpstat -P ALL 5
Linux 3.10.0-693.el7.x86_64 (master) 	2022年02月18日 	_x86_64_	(1 CPU)

10时09分22秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
10时09分27秒  all   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00
10时09分27秒    0   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00

10时09分27秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
10时09分32秒  all   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00
10时09分32秒    0   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00

10时09分32秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
10时09分37秒  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
10时09分37秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00

10时09分37秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
10时09分42秒  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
10时09分42秒    0  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00

```

从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到， 正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。

那么，到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询：

间隔 5 秒后输出一组数据

```
[root@master ~]# pidstat -u 5 1
Linux 3.10.0-693.el7.x86_64 (master) 	2022年02月18日 	_x86_64_	(1 CPU)

10时10分14秒   UID       PID    %usr %system  %guest    %CPU   CPU  Command
10时10分19秒     0       286    0.00    0.20    0.00    0.20     0  xfsaild/sda3
10时10分19秒     0       544    0.00    0.20    0.00    0.20     0  vmtoolsd
10时10分19秒     0      1155  100.00    0.00    0.00  100.00     0  stress
10时10分19秒     0      1156    0.20    0.00    0.00    0.20     0  watch
```



从这里可以明显看到，stress 进程的 CPU 使用率为 100%。

#####  2.4.场景二：I/O密集型进程

首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync：

[root@master ~]# stress -i 1 --timeout 600

还是在第二个终端运行 uptime 查看平均负载的变化情况：

[root@master ~]# watch -d uptime

 ...,	load average: 1.06, 0.58, 0.37

然后，第三个终端运行 mpstat 查看 CPU 使用率的变化情况：

显示所有 CPU 的指标，并在间隔 5 秒输出一组数据

```
[root@master ~]#  mpstat -P ALL 5 1
Linux 3.10.0-693.el7.x86_64 (master) 	2022年02月18日 	_x86_64_	(1 CPU)

10时17分58秒  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
10时18分03秒  all    0.20    0.00   99.80    0.00    0.00    0.00    0.00    0.00    0.00    0.00
10时18分03秒    0    0.20    0.00   99.80    0.00    0.00    0.00    0.00    0.00    0.00    0.00

平均时间:  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
平均时间:  all    0.20    0.00   99.80    0.00    0.00    0.00    0.00    0.00    0.00    0.00
平均时间:    0    0.20    0.00   99.80    0.00    0.00    0.00    0.00    0.00    0.00    0.00

```



从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，而%sys内核 态 高达 99.8%。这说明，平均负载的升高是由于 i/o 的升高。

那么到底是哪个进程，导致%sys 这么高呢？我们还是用 pidstat 来查询：

 间隔 5 秒后输出一组数据，-u 表示 CPU 指标

```
[root@master ~]# pidstat -u 5 1
Linux 3.10.0-693.el7.x86_64 (master) 	2022年02月18日 	_x86_64_	(1 CPU)

10时22分41秒   UID       PID    %usr %system  %guest    %CPU   CPU  Command
10时22分46秒     0       251    0.00    0.40    0.00    0.40     0  kworker/u256:2
10时22分46秒     0      1135    0.00    0.20    0.00    0.20     0  sshd
10时22分46秒     0      1288    0.00    0.20    0.00    0.20     0  kworker/0:1
10时22分46秒     0      1518    0.20   99.20    0.00   99.40     0  stress
10时22分46秒     0      2341    0.00    0.40    0.00    0.40     0  kworker/u256:1

平均时间:   UID       PID    %usr %system  %guest    %CPU   CPU  Command
平均时间:     0       251    0.00    0.40    0.00    0.40     -  kworker/u256:2
平均时间:     0      1135    0.00    0.20    0.00    0.20     -  sshd
平均时间:     0      1288    0.00    0.20    0.00    0.20     -  kworker/0:1
平均时间:     0      1518    0.20   99.20    0.00   99.40     -  stress
平均时间:     0      2341    0.00    0.40    0.00    0.40     -  kworker/u256:1

```

可以发现，还是 stress 进程导致的。

##### 2.5.场景三：大量进程的场景

当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。比如，我们还是使用 stress，但这次模拟的是 8 个进程：

[root@master ~]#  stress -c 8 --timeout 600

由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97：

 uptime

2 ...,	load average: 7.97, 5.93, 3.02

接着再运行 pidstat 来看一下进程的情况：

间隔 5 秒后输出一组数据

```
[root@master ~]#  pidstat -u 5 1
Linux 3.10.0-693.el7.x86_64 (master) 	2022年02月18日 	_x86_64_	(1 CPU)

10时24分15秒   UID       PID    %usr %system  %guest    %CPU   CPU  Command
10时24分20秒     0      2549   12.60    0.00    0.00   12.60     0  stress
10时24分20秒     0      2550   12.60    0.00    0.00   12.60     0  stress
10时24分20秒     0      2551   12.60    0.00    0.00   12.60     0  stress
10时24分20秒     0      2552   12.40    0.00    0.00   12.40     0  stress
10时24分20秒     0      2553   12.40    0.00    0.00   12.40     0  stress
10时24分20秒     0      2554   12.40    0.00    0.00   12.40     0  stress
10时24分20秒     0      2555   12.40    0.00    0.00   12.40     0  stress
10时24分20秒     0      2556   12.60    0.00    0.00   12.60     0  stress

平均时间:   UID       PID    %usr %system  %guest    %CPU   CPU  Command
平均时间:     0      2549   12.60    0.00    0.00   12.60     -  stress
平均时间:     0      2550   12.60    0.00    0.00   12.60     -  stress
平均时间:     0      2551   12.60    0.00    0.00   12.60     -  stress
平均时间:     0      2552   12.40    0.00    0.00   12.40     -  stress
平均时间:     0      2553   12.40    0.00    0.00   12.40     -  stress
平均时间:     0      2554   12.40    0.00    0.00   12.40     -  stress
平均时间:     0      2555   12.40    0.00    0.00   12.40     -  stress
平均时间:     0      2556   12.60    0.00    0.00   12.60     -  stress

```

可以看出，8 个进程在争抢 1 个 CPU。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。

 

### 小结

分析完这三个案例，我再来归纳一下平均负载的理解。

平均负载提供了一个快速查看系统整体性能的手段，反映了整体的负载情况。但只看平均负载本身，我们并不能直接发现，到底是哪里出现了瓶颈。所以，在理解平均负载时，也要注意：

平均负载高有可能是 大量进程运行 导致的；

平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了；

  当 发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源。

### 2、有关磁盘I/O 的调优

2.1.ulimit 资源限制
限制用户资源配置文件：/etc/security/limits.conf 
[root@base1 ~]# vim /etc/security/limits.conf
每行的格式：用户名/@用户组名	类型(软限制/硬限制)	选项	值

**例1、永久修改一个进程可以打开的最大文件数**
[root@base1 ~]# vim /etc/security/limits.conf #在文件的最后追加以下内容
*** soft	nofile	1024000**
*** hard	nofile	1024000**
注：soft 是一个警告值，而 hard 则是一个真正意义的阀值，超过就会报错。一般把 soft 和hard 都配置成一样的值。 最大打开的文件数以文件描叙符 file descripter 计数)。

2、启动系统
[root@base1 ~]# ulimit   -n	#查看一个进程可以打开的文件数，默认是 1024 
[root@base1 ~]# reboot	#想要刚修改的 limits.conf 中配置永久生效，必须重启系统检查：
[root@base1 ~]# ulimit -n 1024000

**方法二：#临时修改**
**[root@base1 ~]# ulimit -n   10000	#不用重启系统。
[root@base1 ~]# ulimit -n**
**10000**

**例 2：配置一个用户可以打开的最大进程数
[root@base1 ~]# vim /etc/security/limits.d/20-nproc.conf 
改：**

| 点击这里 | 点击这里 | 点击这里 | 点击这里 |
| -------- | -------- | -------- | -------- |
| 5 *      | soft     | nproc    | 4096     |
| 为：     |          |          |          |
| 5 *      | soft     | nproc    | 65535    |
| 6 *      | hard     | nproc    | 65535    |

**[root@base1 ~]# ulimit -u	#重启前先查看一下，默认是 3828。**
[root@base1 ~]# reboot	#重启一下。
[root@base1 ~]# ulimit -u
65535
或：
再打一个终端，直接查看
[root@base1 ~]# ulimit -u 

**临时修改：**
[root@base1 ~]# ulimit -u 60000 		
[root@base1 ~]# ulimit -u
60000


atop命令     #查看DSK项参数，了解磁盘I/O问题

![img](https://note-1308251438.cos.ap-guangzhou.myqcloud.com/typora/202204191036351.png)

iostat:磁盘性能查询命令
 语法：iostat 参数 磁盘
 参数
 -c：仅显示CPU使用情况 
 -d：仅显示设备利用率 
 -k：显示状态以kb每秒为单位，而不使用块每秒 
 -m：显示状态以mb每秒为单位 
 -p：仅显示块设备和所有被使用的其他分区的状态 
 -t：显示每个报告产生时的时间 
 -x：显示扩展状态

 [root@home ~]# yum install -y sysstat

 例：统计目前磁盘状态，每秒1次，共3次
 [root@home ~]# iostat  1 3
Linux 3.10.0-693.el7.x86_64 (home) 	2021年08月06日 	_x86_64_	(1 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.04    0.00    0.11    0.04    0.00   99.81

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda               1.25        30.84        10.55     258067      88291
sdb               0.02         0.53         0.00       4404          0
scd0              0.00         0.13         0.00       1046          0

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.00    0.00    0.00    0.00    0.00  100.00

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda               0.00         0.00         0.00          0          0
sdb               0.00         0.00         0.00          0          0
scd0              0.00         0.00         0.00          0          0

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.00    0.00    0.00    0.00    0.00  100.00

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda               0.00         0.00         0.00          0          0
sdb               0.00         0.00         0.00          0          0
scd0              0.00         0.00         0.00          0          0


[root@home 1060]# iostat -kx /dev/sda 
Linux 3.10.0-693.el7.x86_64 (home) 	2021年08月06日 	_x86_64_	(1 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
                0.04    0.00       0.11      0.04       0.00      99.81

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.02    0.80    0.44    30.58    10.46    66.19     0.01    4.67    6.84    0.68   1.49   0.19


%user：应用程序使用CPU的时间占比
%nice：拥有高优先级的应用程序使用CPU的时间占比
%system：内核程序使用CPU的时间占比
%iowait：表示等待进行 I/O 所使用 CPU 的时间百分比
%steal ： 显示虚拟机管理器在服务另一个虚拟处理器时虚拟CPU处在非自愿等待下花费时间的百分比
%idle：显示 CPU 的空闲时间
Device：监测设备名称
rrqm/s：每秒需要读取需求的数量
wrqm/s：每秒需要写入需求的数量
r/s ：每秒实际读取需求的数量
w/s：每秒实际写入需求的数量
rkB/s：每秒实际读取的大小，单位为KB
wkB/s：每秒实际写入的大小，单位为KB
avgrq-sz：平均每次设备I/O操作的数据大小
avgqu-sz：平均I/O队列长度
await：平均每次设备I/O操作的等待时间
svctm：平均每次设备I/O操作的服务时间
%util：被I/O操作消耗的CPU百分比



### 3、有关网络的调优

网卡绑定 Bonding 技术
网卡绑定概述：网卡绑定也称作"网卡捆绑"，就是使用多块物理网卡虚拟成为一块网卡，以提供负载均衡或者冗余，增加带宽的作用。当一个网卡坏掉时，不会影响业务。这个聚合起来的设备看起来是一个单独的以太网接口设备，也就是这几块网卡具有相同的IP地址而并行链接聚合成一个逻辑链路工作。这种技术在Cisco等网络公司中，被称为Trunking和Etherchannel 技术，在Linux的内核中把这种技术称为bonding 。 

#### 3.1 附加NetworkManager命令笔记

在RHEL7中默认使用NetworkManager 守护进程来监控和管理网络设置。nmcli是命令行的管理NetworkManager的工具，会自动把配置写到/etc/sysconfig/network-scripts/目录下面。

1.查看网卡绑定信息
[root@rhel7 ~]# nmcli connection  show 
NAME    UUID                                  TYPE                             DEVICE 
ens33   e1e92a60-31d9-4209-81b3-b00f32be2eee  802-3-ethernet  ens33 
nmcli connection  show = nmcli con show

2.查看网卡连接情况
[root@teach network-scripts]# nmcli d
设备   类型      状态    CONNECTION
ens33  ethernet  连接的  ens33
lo     loopback  未管理  --

2.创建新连接ens33-static，IP自动通过手动获取
con add — 添加新的连接
con-name -连接名
type – 设备类型
ifname – 接口名
autoconnect yes — 允许开机自动启动
gw4 192.168.88.2 –自己的网关
ipv4.dns 114.114.114.114  
[root@teach network-scripts]# nmcli con add con-name ens33-static ifname ens33 autoconnect yes type ethernet  ip4 192.168.1.150/16 gw4 192.168.1.2 
成功添加的连接 'ens33-static'（bb448ef2-d20a-48bb-911c-a06b746d1eb9）。
[root@teach network-scripts]# nmcli con show
名称             UUID                                                          类型                  设备
ens33          054e83e4-cd51-4f09-af1e-372119ae65e5      802-3-ethernet    ens33
ens33-static  bb448ef2-d20a-48bb-911c-a06b746d1eb9     802-3-ethernet    --
[root@teach network-scripts]# nmcli con up ens33-static
[root@teach network-scripts]#ping [www.baidu.com](http://www.baidu.com)

3.查看ens33-static IP
[root@teach network-scripts]#ifconfig

4.再次查看网卡连接情况
[root@teach network-scripts]# nmcli d
设备   类型      状态    CONNECTION
ens33  ethernet  连接的  ens33-static
lo     loopback  未管理  --

4.创建新连接ens33-dhcp，IP自动通过自动获取
[root@rhel7 ~]# nmcli con add con-name ens33-dhcp type ethernet ifname ens33 autoconnect no


5.取消ens33-static当前连接
[root@teach network-scripts]#nmcli con down ens33-static

6.启动ens33-dhcp链接
[root@teach network-scripts]#nmcli con up ens33-dhcp

删除ens33-static
[root@teach network-scripts]#nmcli con del ens33-static

7.修改现存的会话

1）、关闭会话ens33-static的自动连接（autoconnect）。

[root@teach network-scripts]# nmcli con modify  ens33-static connection.autoconnect no

2）、修改会话ens33-static的DNS服务器地址

[root@teach network-scripts]# nmcli con modify  ens33-static ipv4.dns 8.8.8.8

3）、有一些配置参数，是可以添加和删除的，比如使用+ 或 - 号在参数前面。比如添加第二个DNS服务器地址

[root@teach network-scripts]# nmcli con modify  ens33-static +ipv4.dns 8.8.4.4

4）、更换静态IP地址和默认网关。 

[root@teach network-scripts]# nmcli con modify ens33-static ipv4.addresses 192.168.0.120/24 ipv4.gateway 192.168.0.1

5）、添加第二个ip

[root@teach network-scripts]# nmcli con modify ens33-static  +ipv4.addresses 192.168.0.130/24 



 注意：nmcli con modify 修改的配置，会自动保存成配置文件，并且重启后依然有效，但是如果配置更改了，你需要从新激活一下，使新配置生效。

[root@teach network-scripts]# nmcli con up ens33-static

#### 3.2  Bonding 技术分类 

1. 负载均衡
   对于bonding的网络负载均衡是我们在文件服务器中常用到的，比如把三块网卡，当做一块来用，解决一个IP地址，流量过大，服务器网络压力过大的问题。为了解决同一个IP地址，突破流量的限制，毕竟     网线和网卡对数据的吞吐量是有限制的。如果在有限的资源的情况下，实现网络负载均衡，最好的办法就     是 bonding 。
2. 网络冗余
   对于服务器来说，网络设备的稳定也是比较重要的，特别是网卡。在生产型的系统中，网卡的可靠性    就更为重要了。在生产型的系统中，大多通过硬件设备的冗余来提供服务器的可靠性和安全性，比如电源。bonding 也能为网卡提供冗余的支持。把多块网卡绑定到一个IP地址，当一块网卡发生物理性损坏的情况下，另一块网卡自动启用，并提供正常的服务，即：默认情况下只有一块网卡工作，其它网卡做备份。



#### 3.3 	实战- 配置多网卡绑定技术

1、base1 配置两个网卡，添加网卡如下图：

**![image-20220419113321673](https://note-1308251438.cos.ap-guangzhou.myqcloud.com/typora/202204191133728.png)**



<font color='red'>注：添加一张网卡时，ifconfig没有显示，因为没有生成新网卡的配置文件，需要添加新网卡的配置文件，ifconfig才能有显示</font>

启动系统后查看当前网卡信息：

```
[root@master ~]# ifconfig
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.245.204  netmask 255.255.255.0  broadcast 192.168.245.255
        inet6 fe80::20c:29ff:fe51:5358  prefixlen 64  scopeid 0x20<link>
        ether 00:0c:29:51:53:58  txqueuelen 1000  (Ethernet)
        RX packets 24145  bytes 24043496 (22.9 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15186  bytes 5469822 (5.2 MiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

ens37: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.245.209  netmask 255.255.255.0  broadcast 192.168.245.255
        inet6 fe80::758a:699b:c013:ebfc  prefixlen 64  scopeid 0x20<link>
        ether 00:0c:29:51:53:62  txqueuelen 1000  (Ethernet)
        RX packets 23  bytes 1946 (1.9 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 11  bytes 1368 (1.3 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

```


实验目标：绑定网卡：ens33+ens37=bond0。实现 active-backup 主备模式，即一个网卡处于活跃状态，另一个处于备份状态，当活跃网卡down 掉时，启用备份网卡。设置网卡 ens33 为主网卡(优先处于活跃状态)，ens37 为辅网卡(备份状态，主网卡链路正常时，辅网卡处于备份状态)。

1、移除网卡配置文件
[root@base1 ~]# cd /etc/sysconfig/network-scripts/ 

[root@base1 network-scripts]# mkdir ./net_bak
[root@base1 network-scripts]# mv ifcfg-ens* ./net_bak/	#有了band0 配置文件后， 就不需要使用原来的网卡配置文件了

2、使用 nmcli 命令配置生成bond0 配置文件

方法1：

[root@base1 network-scripts]# nmcli connection add type bond ifname bond0 con-name bond0 miimon 100 mode active-backup primary ens33 ip4 192.168.1.63/24  ip4.gateway 192.168.1.1

参数说明：
add type bond ifname bond0 #添加一个类型为bond，网卡名为bond0 的设备
con-name bond0 miimon 100 #配置 name 为bond0 的链路监控的频率为 100 毫秒。mode active-backup	#指定bond0 模式为active-backup（主动备份） primary ens33	#指定主网卡为ens33
ip4 192.168.1.63/24	#指定IP 地址为 192.168.1.63/24

配置完成后，此时会在/etc/sysconfig/network-scripts 目录下生成 ifcfg-bond0 的配置文件ifcfg-bond0：
[root@base1 network-scripts]# cd /etc/sysconfig/network-scripts 

方法2：

```
[root@master network-scripts]# vim ifcfg-bond0
BONDING_OPTS="miimon=100 mode=active-backup primary=ens33"
TYPE=Bond
BONDING_MASTER=yes
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
IPADDR=192.168.245.131
PREFIX=24
GATEWAY=192.168.245.2
DEFROUTE=yes
NAME=bond0
DEVICE=bond0
ONBOOT=yes

```

3、将网卡ens33 绑定到bond0，即将网卡ens33创建为bond0 的子接口
[root@base1 ~]# nmcli connection add type bond-slave ifname ens33 master bond0 

```
[root@master network-scripts]# vim ifcfg-bond-slave-ens33
TYPE=Ethernet
NAME=bond-slave-ens33
DEVICE=ens33
ONBOOT=yes
MASTER=bond0
SLAVE=yes
```

[root@base1 ~]# nmcli connection add type bond-slave ifname ens37 master bond0

```
[root@master network-scripts]# vim ifcfg-bond-slave-ens37
TYPE=Ethernet
NAME=bond-slave-ens37
DEVICE=ens37
ONBOOT=yes
MASTER=bond0
SLAVE=yes

```

\#装网卡ens37 绑定到bond0
注：指定ens33 和ens37 的设备类型为bond-slave，绑定到master bond0 上

4、查看生成的配置文件
[root@base1 ~]# ls /etc/sysconfig/network-scripts/ifcfg-bond*
/etc/sysconfig/network-scripts/ifcfg-bond0
/etc/sysconfig/network-scripts/ifcfg-bond-slave-ens33
/etc/sysconfig/network-scripts/ifcfg-bond-slave-ens37
[root@base1 ~]# systemctl restart network	#重启网络服务，使用之前的 bond 相关的配置生效。

5、查看当前已激活的网络接口

```
[root@base1 ~]# nmcli connection show --active
名称              UUID                                  类型            设备  
bond-slave-ens33  e2103c72-60fa-46ce-be56-76ff0a35f66f  802-3-ethernet  ens33 
bond-slave-ens37  785de252-b76c-4685-8e04-5185ed0cab15  802-3-ethernet  ens37 
bond0             6b082fe0-7352-4c5e-ad8e-c3784f0936b3  bond            bond0
```

6、查看bond0 当前状态：

```
[root@master network-scripts]# cat /proc/net/bonding/bond0
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: fault-tolerance (active-backup)
Primary Slave: ens33 (primary_reselect always)
Currently Active Slave: ens33
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0

Slave Interface: ens33
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 00:0c:29:51:53:58
Slave queue ID: 0

Slave Interface: ens37
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 00:0c:29:51:53:62
Slave queue ID: 0

```



7、测试bonding 主备高可用切换
[root@base1 ~]# ping 192.168.245.2	#打开终端后，持续ping 网关



此时断开虚拟机ens33网络连接，断开后，发现ping没有丢包，没有问题。说明多网卡绑定成功

![image-20220218104557732](https://s2.loli.net/2022/02/18/tq9yXNxvIjRg6Y7.png)



8、查看网卡连接

```
[root@master network-scripts]# cat /proc/net/bonding/bond0
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: fault-tolerance (active-backup)
Primary Slave: ens33 (primary_reselect always)
Currently Active Slave: ens37
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0

Slave Interface: ens33
MII Status: down
Speed: Unknown
Duplex: Unknown
Link Failure Count: 1
Permanent HW addr: 00:0c:29:51:53:58
Slave queue ID: 0

Slave Interface: ens37
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 00:0c:29:51:53:62
Slave queue ID: 0

```

MII Status: down	#已经down 了

#### 3.4 了解一下mode的七种模式

常用的模式mode 是 0、1、4、6 这几种模式。具体如下： 
balance-rr (0) –轮询模式，负载均衡（bond 默认的模式） 
active-backup (1) –主备模式（常用）
balance-xor (2) 
broadcast (3） 
802.3ad (4) –聚合模式
balance-tlb (5)
balance-alb (6) 

详情如下：
**mode=0**  #默认是mode=0, 有高可用 (容错) 和负载均衡的功能, 需要交换机的配置，每块网卡轮询发包 (流量分发比较均衡)，mode 0 下bond 所绑定的网卡的IP 都被修改成相同的mac 地址，如果这些网卡都被接在同一个交换机，那么交换机的 arp 表里这个 mac 地址对应的端口就有多个，那么交换机接受到发往这个mac 地址的包应该往哪个端口转发呢？正常情况下 mac 地址是全球唯一的，一个 mac 地址对应多个端口肯定使交换机迷惑了。所以 mode0 下的bond 如果连接到交换机，交换机这几个端口应该采取聚合方式（cisco 称为ethernetchannel），因为交换机做了聚合后，聚合下的几个端口也被捆绑成一个mac 地址。若我们不配置，我们的解 决办法是，两个网卡接入不同的交换机即可。
**mode=1**  #只有一个设备处于活动状态，当一个宕掉另一个马上由备份转换为主设备。mac 地址是外部可见得，从外面看来，bond 的 MAC 地址是唯一的，以避免 switch(交换机)发生混乱。此模式只提供了容错能力；由此可见此算法的优点是可以提供高网络连接的可用性，但是它的资源利用率较低，只

有一个接口处于工作状态，在有 N 个网络接口的情况下，资源利用率为 1/N
**mode=2** #基于指定的传输 HASH 策略传输数据包。缺省的策略是：(源 MAC 地址 XOR 目标MAC 地址) % slave 数量。其他的传输策略可以通过xmit_hash_policy 选项指定，此模式提供负载平衡和容错能力。
**mode=3** #在每个 slave 接口上传输每个数据包，此模式提供了容错能力，非常不常用
**mode=4** #创建一个聚合组，它们共享同样的速率和双工设定。根据 802.3ad 规范将多个 slave 工作在同一个激活的聚合体下。外出流量的 slave 选举是基于传输 hash 策略， 该策略可以通过xmit_hash_policy 选项从缺省的XOR 策略改变到其他策略。需要几个必要条件：
ethtool 支持获取每个 slave 的速率和双工设定
switch(交换机)支持IEEE 802.3ad Dynamic link aggregation 大多数 switch(交换机)需要经过特定配置才能支持 802.3ad 模式
**mode=5**  #不需要任何特别的switch(交换机)支持的通道bonding。在每个slave 上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的 slave 出故障了，另一个 slave 接管失败的slave 的MAC 地址。该模式的必要条件：ethtool 支持获取每个 slave 的速率
**mode=6** #模式包含了balance-tlb 模式，同时加上针对 IPV4 流量的接收负载均衡(receive load balance, rlb)，而且不需要任何 switch(交换机)的支持。接收负载均衡是通过 ARP 协商实现的。
bonding 驱动截获本机发送的 ARP 应答，并把源硬件地址改写为 bond 中某个 slave 的唯一硬件地址， 从而使得不同的对端使用不同的硬件地址进行通信。来自服务器端的接收流量也会被均衡。当本机发送ARP 请求时，bonding 驱动把对端的 IP 信息从ARP 包中复制并保存下来。当 ARP 应答从对端到达  时， bonding 驱动把它的硬件地址提取出来，并发起一个 ARP 应答给bond 中的某个 slave。使用 ARP 协商进行负载均衡的一个问题是：每次广播 ARP 请求时都会使用 bond 的硬件地址，因此对端学习到这个硬件地址后，接收流量将会全部流向当前的 slave。这个问题可以通过给所有的对端发送更新 （ARP 应答） 来解决，应答中包含他们独一无二的硬件地址，从而导致流量重新分布。当新的 slave 加入到 bond 中时， 或者某个未激活的 slave 重新 激活时，接收流量也要重新分布。接收的负载被顺序地分布（round robin） 在bond 中最高速的 slave 上当某个链路被重新接上，或者一个新的 slave 加入到bond 中，接收流量在所有当前激活的 slave 中全部重新分配，通过使用指定的 MAC 地址给每个 client 起 ARP 应答。下面介绍的 updelay 参数必须被设置为某个大于等于 switch(交换机)转发延时的值，从而保证发往对端的 ARP 应答 不会被switch(交换机)阻截。

### 4、有关软件与内核参数的调优

#### 1.关闭不需要的服务

ntsysv
yum install -y ntsysv

#### 2.关闭不需要的tty

vi /etc/systemd/logind.conf
[Login]
改：
#NAutoVTs=6
为：
NAutoVTs=2

关闭已开终端
systemctl stop getty@tty6.service

#### 3.tcp/ip网络参数进行调整 

echo 'net.ipv4.tcp_syncookies = 1 '   >>/etc/sysctl.conf

net.ipv4.tcp_syncookies = 1
表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；

sysctl -p	  sysctl命令用于运行时配置内核参数，这些参数位于/proc/sys目录下。
sysctl配置与显示在/proc/sys目录中的内核参数．可以用sysctl来设置或重新设置联网功能，如IP转发、IP碎片去除以及源路由检查等。
用户只需要编辑/etc/sysctl.conf文件，即可手工或自动执行由sysctl控制的功能。
  -p   从指定的文件加载系统参数，如不指定即从/etc/sysctl.conf中加载


 net.ipv4.tcp_syncookies = 1
表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；

net.ipv4.tcp_tw_reuse = 1
表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；

net.ipv4.tcp_tw_recycle = 1
表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。

net.ipv4.tcp_fin_timeout = 30
表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。

net.ipv4.tcp_keepalive_time = 1200
表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。

net.ipv4.ip_local_port_range = 1024 65000
表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。

net.ipv4.tcp_max_syn_backlog = 8192
表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。

net.ipv4.tcp_max_tw_buckets = 5000
表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。
默认为180000，改为 5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。
此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死

net.ipv6.conf.all.disable_ipv6=1
禁用IPV6

注意：禁用IPV6后，可能会导致某些服务无法启动,比如VSFTP，对于VSFTP，需要修改/etc/vsftpd/vsftpd.conf文件中的listen和listen_ipv6两个选项：

listen=YES
listen_ipv6=NO

#### 4.限制root用户远程登录

打开/etc/ssh/sshd_config文件，找到PermitRootLogin参数，将yes改为no，不同系统版本可能这个PermitRootLogin参数的位置不一样，不过操作是一样的，都是改为no。
vim /etc/ssh/sshd_config